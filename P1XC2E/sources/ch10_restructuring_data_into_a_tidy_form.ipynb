{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "@File         : ch10_restructuring_data_into_a_tidy_form.ipynb\n",
    "@Author(s)    : Stephen CUI\n",
    "@LastEditor(s): Stephen CUI\n",
    "@CreatedTime  : 2024-07-25 15:11:41\n",
    "@Email        : cuixuanstephen@gmail.com\n",
    "@Description  : 将数据重组为整洁的形式\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是整洁数据？Hadley 提出了三个确定数据集是否整洁的指导原则：\n",
    "\n",
    "1. 每个变量形成一列\n",
    "2. 每个观察值形成一行\n",
    "3. 每种类型的观察单位形成一个表格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决混乱数据的第一步是识别它，因为它存在，而且有无限的可能性。哈德利明确提到了五种最常见的混乱数据类型：\n",
    "1. 列名是值，而不是变量名\n",
    "2. 多个变量存储在列名中\n",
    "3. 变量存储在行和列中\n",
    "4. 多种类型的观测单元存储在同一张表中\n",
    "5. 单个观察单元存储在多个表中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 `stack` 将变量值整理为列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fruit = pd.read_csv('data/state_fruit.csv', index_col=0)\n",
    "state_fruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这张表看起来并没有什么杂乱之处，信息也很容易理解。然而，根据整洁原则，它并不整洁。每个列名都是一个变量的值。事实上，DataFrame 中甚至没有一个变量名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fruit.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    state_fruit.stack()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    state_fruit.stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={'level_0': 'state', 'level_1': 'fruit', 0: 'weight'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    state_fruit.stack()\n",
    "    .rename_axis(['state', 'fruit'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    state_fruit.stack()\n",
    "    .rename_axis(['state', 'fruit'])\n",
    "    .reset_index(name='weight')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.stack` 默认情况下，它会获取（层次结构列中最内层的）列名并对其进行转置，这样它们就成为新的最内层索引级别。请注意，每个旧列名仍通过与每个州配对来标记其原始值。3 x 3 DataFrame 中有 9 个原始值，它们被转换为具有相同数量值的单个 Series。原始的第一行数据成为结果 Series 中的前三个值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `.stack` 的关键之一是将您不想转换的所有列放在索引中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fruit2 = pd.read_csv('data/state_fruit2.csv')\n",
    "state_fruit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fruit2.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fruit2.set_index('State').stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要正确重塑这些数据，您需要先使用 `.set_index` 方法将所有未重塑的列放入索引中，然后使用 `.stack`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `melt` 将变量值整理为列名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame 有一个名为 `.melt` 的方法，它类似于上一节中描述的 `.stack` 方法，但灵活性更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fruit2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `.melt` 方法，将适当的列传递给 `id_vars` 和 `value_vars` 参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fruit2.melt(id_vars=['State'], value_vars=['Apple', 'Orange', 'Banana'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，`.melt` 将转换后的列名称称为变量，将相应的值称为值。方便的是，`.melt` 有两个附加参数，`var_name` 和 `value_name`，可让您重命名这两列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fruit2.melt(id_vars=['State'],\n",
    "                  value_vars=['Apple', 'Orange', 'Banana'],\n",
    "                  var_name='fruit',\n",
    "                  value_name='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.melt` 方法重塑您的 DataFrame。它最多需要五个参数，其中两个对于理解如何正确重塑数据至关重要："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `id_vars` 是要保留为列而不是 reshape\n",
    "- `value_vars` 是要重塑为单个列的列名列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id_vars （即标识变量）保留在同一列中，但对传递给value_vars 的每个列重复。`.melt` 的一个重要方面是它会忽略索引中的值，它会默默地删除索引并将其替换为默认的RangeIndex。这意味着，如果您的索引中确实有您想要保留的值，则在使用 `melt` 之前，您需要先重置索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fruit2.melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fruit2.melt(id_vars='State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时 `stack` 多组变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = pd.read_csv('data/movie.csv')\n",
    "actor = movie[['movie_title', 'actor_1_name', 'actor_2_name',\n",
    "               'actor_3_name', 'actor_1_facebook_likes', 'actor_2_facebook_likes',\n",
    "               'actor_3_facebook_likes']]\n",
    "actor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_col_name(col_name):\n",
    "    col_name = col_name.replace('_name', '')\n",
    "    if 'facebook' in col_name:\n",
    "        fb_idx = col_name.find('facebook')\n",
    "        col_name = (col_name[:5] + col_name[fb_idx - 1:]\n",
    "                    + col_name[5: fb_idx - 1])\n",
    "    return col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor2 = actor.rename(columns=change_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stubs = ['actor', 'actor_facebook_likes']\n",
    "actor2_tidy = pd.wide_to_long(\n",
    "    actor2, stubnames=stubs, i=['movie_title'], j='actor_num',\n",
    "    sep=\"_\"\n",
    ")\n",
    "actor2_tidy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wide_to_long` 函数的工作方式相当特殊。其主要参数是 `stubnames`，即字符串列表。每个字符串代表一个列分组。所有以此字符串开头的列都将堆叠到单个列中。在此配方中，有两组列：actor 和 actor_facebook_likes。默认情况下，这些列组中的每一组都需要以数字结尾。随后将使用此数字来标记重塑后的数据。这些列组中的每一组都有一个下划线字符，将 `stubname` 与结尾数字分隔开。为此，您必须使用 `sep` 参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除此之外，`wide_to_long` 需要一个唯一的列，即参数 `i`，作为不会被堆叠的标识变量。还需要参数 `j`，它重命名从原始列名末尾剥离的标识数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/stackme.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'a1':'group1_a1', 'b2':'group1_b2',\n",
    "                     'd':'group2_a1', 'e':'group2_b2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.wide_to_long(\n",
    "    df.rename(columns = {'a1':'group1_a1', 'b2':'group1_b2',\n",
    "                     'd':'group2_a1', 'e':'group2_b2'}),\n",
    "    stubnames=['group1', 'group2'],\n",
    "    i=['State', 'Country', 'Test'],\n",
    "    j='Label', suffix='.+', sep='_'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反转堆叠数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames 有两个类似的方法，`.stack` 和 `.melt`，用于将水平列名称转换为垂直列值。DataFrames 可以分别使用 `.unstack` 和 `.pivot` 方法反转这两个操作。`.stack` 和 `.unstack` 是仅允许控制列和行索引的方法，而 `.melt` 和 `.pivot` 可以更灵活地选择要重塑哪些列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usecol_func(name):\n",
    "    return 'UGDS_' in name or name == 'INSTNM'\n",
    "\n",
    "college = pd.read_csv('data/college.csv',\n",
    "                      index_col='INSTNM', usecols=usecol_func)\n",
    "college"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_stacked = college.stack()\n",
    "college_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_stacked.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college2 = pd.read_csv('data/college.csv', usecols=usecol_func)\n",
    "college2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_melted = college2.melt(id_vars='INSTNM', var_name='Race', \n",
    "                               value_name='Percentage')\n",
    "college_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_melted.pivot(index='INSTNM', columns='Race', values='Percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college.stack().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分组聚合后取消堆叠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按单个列对数据进行分组并对单个列执行聚合将返回易于使用的结果。按多个列进行分组时，生成的聚合可能不以易于使用的方式构建。由于 `.groupby` 操作默认将唯一的分组列放在索引中，因此 `.unstack` 方法可以有利于重新排列数据，以便以更有利于解释的方式呈现数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee = pd.read_csv('data/employee.csv')\n",
    "(\n",
    "    employee.groupby('RACE')['BASE_SALARY']\n",
    "    .mean()\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    employee.groupby(['RACE', 'GENDER'])['BASE_SALARY']\n",
    "    .mean()\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    employee.groupby(['RACE', 'GENDER'])['BASE_SALARY']\n",
    "    .mean()\n",
    "    .astype(int)\n",
    "    .unstack()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    employee.groupby(['RACE', 'GENDER'])['BASE_SALARY']\n",
    "    .mean()\n",
    "    .astype(int)\n",
    "    .unstack('RACE')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    employee.groupby(['RACE', 'GENDER'])['BASE_SALARY']\n",
    "    .agg(['mean', 'max', 'min'])\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    employee.groupby(['RACE', 'GENDER'])['BASE_SALARY']\n",
    "    .agg(['mean', 'max', 'min'])\n",
    "    .astype(int)\n",
    "    .unstack('GENDER')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    employee.groupby(['RACE', 'GENDER'])['BASE_SALARY']\n",
    "    .agg(['mean', 'max', 'min'])\n",
    "    .astype(int)\n",
    "    .unstack('RACE')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
