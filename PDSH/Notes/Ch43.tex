\chapter{In Depth: Support Vector Machines\label{Ch43}}
Support vector machines (SVMs) are a particularly powerful and flexible class of
supervised algorithms for both classification and regression.
\section{Motivating Support Vector Machines}
As part of our discussion of Bayesian classification (see \autoref{Ch41}), we learned about a simple kind of model that describes the distribution of each underlying class, and experimented with using it to probabilistically determine labels for new points. That was an example of generative classification; here we will consider instead discriminative classification. That is, rather than modeling each class, we will simply find a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other.

\figures{Three perfect linear discriminative classifiers for our data}

These are three very different separators which, nevertheless, perfectly discriminate between these samples. Depending on which you choose, a new data point (e.g., the one marked by the “X” in \autoref{Three perfect linear discriminative classifiers for our data}) will be assigned a different label! Evidently our simple intuition of “drawing a line between classes” is not good enough, and we need to think a bit more deeply.
\section{Support Vector Machines: Maximizing the Margin}
Support vector machines offer one way to improve on this. The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a \textit{margin}\marginpar[margin]{margin} of some width, up to the nearest point. \important{The line that maximizes this margin is the one we will choose as the optimal model.}

\subsection*{Fitting a Support Vector Machine}
Let’s see the result of an actual fit to this data: we will use Scikit-Learn’s support vector classifier (SVC) to train an SVM model on this data.

A key to this classifier’s success is that for the fit, only the positions of the support vectors matter; any points further from the margin that are on the correct side do not modify the fit. Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.

\figures{The influence of new training points on the SVM model}

\autoref{The influence of new training points on the SVM model}, in the left panel, we see the model and the support vectors for 60 training points. In
the right panel, we have doubled the number of training points, but the model has
not changed: the three support vectors in the left panel are the same as the support
vectors in the right panel. \important{This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model.}
\subsection*{Beyond Linear Boundaries: Kernel SVM}
Where SVM can become quite powerful is when it is combined with \textit{kernels}. There we projected our data into a higher-dimensional space defined by polynomials
and Gaussian basis functions, and thereby were able to fit for nonlinear relationships
with a linear classifier.
\subsection*{Tuning the SVM: Softening Margins}


